# An Investigation of Hybrid Recommendation Methods on Music Listening Data

## Motivation

Recommendation systems are non-trivial. While they dominate industrial applications in media, e-commerce, and content platforms, their behavior under constrained data conditions and their sensitivity to algorithmic assumptions remain underexplored in many educational and applied contexts.

This project investigates trade-offs between content-based filtering (CBF) and collaborative filtering (CF) approaches, specifically:

- **Content-Based Filtering (CBF)** assumes item similarity can be inferred from metadata or semantic embeddings, independent of user behavior. It performs well for cold-start items but ignores user-item interaction patterns.
- **Collaborative Filtering (CF)** assumes user preferences can be inferred from behavioral patterns across users. It captures implicit taste without item knowledge but suffers under sparse interaction data.
- **Hybrid methods** attempt to combine both, but whether this combination meaningfully outperforms baselines is not guaranteed and depends on data structure, sparsity, and weighting strategies.

The core uncertainty this project addresses is: **under what conditions does each method fail, and does a hybrid approach offer interpretable improvement?**

## Core Questions

1. How does content-based filtering behave when user interaction data is sparse or absent?
2. Under what conditions does collaborative filtering degrade, and how does sparsity affect its ability to generalize?
3. Does a hybrid system provide measurable advantage over single-method baselines, or does it simply average mediocrity?
4. What are the failure modes of each approach, and how can they be probed empirically?

## Methodology

### Data

The system uses a small-scale music listening dataset consisting of:
- User-track interaction logs (play history)
- Track metadata (artist, album, genre)
- Pre-computed semantic embeddings (BGE-small-en-v1.5)

The dataset is intentionally limited in size and interaction density to simulate real-world constraints such as cold-start problems and sparse user feedback.

### Models

#### Content-Based Filtering (CBF)
Constructs a track similarity matrix using cosine similarity of semantic embeddings derived from track metadata. Recommendations are generated by averaging embeddings of user listening history and retrieving nearest neighbors.

**Assumptions:**
- Semantic similarity in embedding space correlates with user preference
- User taste is stable across listening sessions

#### Collaborative Filtering (CF)
Implements neural collaborative filtering (NCF) using matrix factorization with learned user and item embeddings. Recommendations are generated via dot product scoring of user-item latent factors.

**Assumptions:**
- User-item interactions encode latent preference structure
- Sufficient interaction density exists to learn generalizable patterns

#### Hybrid System
Combines CBF and CF scores using weighted averaging. Falls back to CBF-only when CF cannot produce recommendations (e.g., new users).

**Assumptions:**
- Both methods provide complementary signal
- Linear combination is sufficient for fusion

### Baselines

The evaluation compares:
- CBF-only recommendations
- CF-only recommendations
- Hybrid recommendations
- Implicit popularity baseline (not fully implemented)

## Evaluation Philosophy

### Metrics

Standard offline metrics are used:
- **Precision@k**: Proportion of recommended items that appear in test set
- **Recall@k**: Proportion of test items successfully retrieved
- **NDCG@k**: Normalized discounted cumulative gain (rank-aware relevance)
- **Hit Rate@k**: Fraction of users with at least one correct recommendation

Evaluation uses temporal train/test splitting to simulate realistic prediction scenarios.

### What is NOT Measured

- **Online metrics**: No A/B testing, user engagement, or click-through rates
- **Diversity or novelty**: Only top-k accuracy is measured
- **Fairness or bias**: No demographic or item exposure analysis
- **Scalability**: Evaluation is limited to <100 users

### Known Limitations

The evaluation results (see `evaluation_results/`) reveal critical limitations:

- **CBF-only performance**: 0.0 precision/recall across all k values, indicating either severe data sparsity or embedding misalignment with user taste
- **CF-only performance**: 0.0 precision/recall, likely due to insufficient interaction density for matrix factorization to generalize
- **Hybrid performance**: Marginal improvement at k=20 (precision: 0.00056, recall: 0.0028, NDCG: 0.0012), but still functionally poor

These results suggest:
- The dataset may be too sparse for collaborative methods to learn meaningful patterns
- Content embeddings do not align well with actual listening behavior
- The hybrid weighting strategy does not meaningfully combine signal from both methods

**Cold-start behavior** was not systematically evaluated but is expected to favor CBF.

## Project Structure

The codebase is organized by analysis flow:

```
data/                          # Raw and processed interaction data
models/
  ├── content/                 # CBF similarity matrices and embeddings
  ├── collaborative/           # CF model weights and user/item mappings
  └── hybrid/                  # Hybrid recommender implementation
scripts/
  ├── embed_all_tracks.py      # Generate semantic embeddings
  ├── build_cbf_model.py       # Construct similarity matrix
  ├── train_cf_model.py        # Train neural CF model
  ├── evaluate_models.py       # Offline evaluation harness
  └── visualize_evaluation.py  # Generate comparison plots
notebooks/
  ├── 01_eda.ipynb             # Exploratory data analysis
  ├── 02_embedding_tests.ipynb # (placeholder)
  └── 03_cf_training.ipynb     # (placeholder)
evaluation_results/            # Saved evaluation outputs (JSON, CSV)
mousiki_cli.py                 # CLI utility for probing model behavior
```

The CLI exists to **probe model behavior interactively**, not as a demonstration of usability.

## Reproducibility and Usage

### Setup

```bash
./setup.sh
```

Creates a virtual environment, installs dependencies, and verifies model weights.

### Running Evaluation

To reproduce the offline evaluation:

```bash
python scripts/evaluate_models.py
```

Results will be saved to `evaluation_results/` with a timestamp.

### CLI Probing Utility

The CLI allows manual inspection of recommendations for specific users:

```bash
# Check setup status
python mousiki_cli.py setup

# List available user IDs
python mousiki_cli.py list-users --limit 20

# Generate recommendations for user 1
python mousiki_cli.py recommend --user-id 1 --n 10

# Test CBF-only mode
python mousiki_cli.py recommend --user-id 1 --n 10 --no-cf

# Override user history
python mousiki_cli.py recommend --user-id 1 --history "123,456,789"
```

The CLI is intentionally minimal and serves as a diagnostic tool, not a polished interface.

## Scope and Limitations

### What This Project Does NOT Attempt

- **Production deployment**: No API, no database integration, no scalability considerations
- **Real-time recommendations**: Offline evaluation only
- **Feature engineering**: Minimal metadata usage, no temporal features or context-aware modeling
- **Advanced hybrid methods**: No neural hybrid architectures, meta-learning, or reinforcement learning
- **Comprehensive hyperparameter tuning**: Models use default or lightly tuned parameters

### Why This is Not Production-Ready

The system intentionally omits:
- User authentication and session management
- Real-time inference optimization
- Model monitoring and retraining pipelines
- Data validation and quality assurance
- Extensive logging and error handling

These are not oversights but deliberate scope constraints to focus on algorithmic investigation rather than engineering infrastructure.

### Future Investigation

Open questions for further exploration:
- How does interaction density threshold affect CF performance?
- Can attention-based hybrid methods outperform linear combination?
- What role does embedding model choice play in CBF effectiveness?
- How do cold-start users behave under different fallback strategies?
- Can side information (temporal, contextual) improve sparse-data performance?

## License

MIT
